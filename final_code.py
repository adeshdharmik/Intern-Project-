# -*- coding: utf-8 -*-
"""Final Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RnJDWBOOQ5uRMO0s6WlZUPmPMcZzE0r7
"""

#Thyroid Cancer Risk Prediction Pipeline
# This script implements a complete machine learning pipeline for predicting thyroid cancer risk levels
# (Low, Intermediate, High) using the UCI Differentiated Thyroid Cancer Recurrence Dataset.
# Dataset: Borzooei, S., & Tarokhian, A. (2023). DOI: 10.24432/C5632J.
# Target: Risk (multiclass: Low, Intermediate, High).
# Environment: Google Colab, Python 3.
# Instructions: Upload 'Thyroid_Diff.csv' to /content/ in Colab before running.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
import pickle
import joblib
import shap
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Install required libraries
try:
    import shap
except ImportError:
    os.system('pip install pandas seaborn matplotlib scikit-learn shap joblib')

# Set random seed
np.random.seed(42)

# Create directories for artifacts
os.makedirs('/content/Thyroid_Models', exist_ok=True)
os.makedirs('/content/Thyroid_Plots', exist_ok=True)

# Load dataset
data_path = '/content/Thyroid_Diff.csv'
try:
    df = pd.read_csv(data_path)
    print('Dataset loaded successfully. Shape:', df.shape)
    print('Columns:', df.columns.tolist())
except Exception as e:
    print(f'Error loading dataset: {e}')
    print('Ensure Thyroid_Diff.csv is uploaded to /content/')
    exit()

# Validate data
print('\nMissing Values:\n', df.isna().sum())
print('\nData Types:\n', df.dtypes)
print('\nFirst 5 Rows:\n', df.head())

# Exploratory Data Analysis (EDA)
# Target distribution
print('\nTarget Distribution:\n', df['Risk'].value_counts())
print('\nNormalized Target Distribution:\n', df['Risk'].value_counts(normalize=True))

# Plot target distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='Risk', data=df, order=['Low', 'Intermediate', 'High'])
plt.title('Distribution of Risk Levels')
plt.xlabel('Risk Level')
plt.ylabel('Count')
plt.savefig('/content/Thyroid_Plots/risk_distribution.png', dpi=300)
plt.show()

# Unique values for categorical columns
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    print(f'\n{col} unique values: {df[col].unique()}')

# Data Preprocessing
# Define target and features
target = 'Risk'
features = [col for col in df.columns if col not in [target, 'Response', 'Recurred']]
print('\nFeatures:', features)

# Separate features and target
X = df[features].copy()
y = df[target].copy()

# Encode target
le_target = LabelEncoder()
y = le_target.fit_transform(y)
label_encoders = {target: le_target}

# Encode categorical features
categorical_cols = X.select_dtypes(include='object').columns
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Normalize numerical feature (Age)
scaler = MinMaxScaler()
X['Age'] = scaler.fit_transform(X[['Age']])

# Save encoders and scaler
with open('/content/Thyroid_Models/label_encoders.pkl', 'wb') as f:
    pickle.dump(label_encoders, f)
joblib.dump(scaler, '/content/Thyroid_Models/scaler.pkl')

# Correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(X.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.tight_layout()
plt.savefig('/content/Thyroid_Plots/correlation_heatmap.png', dpi=300)
plt.show()

# Save encoded dataset
X['Risk'] = y
X.to_csv('/content/Thyroid_Models/encoded_dataset.csv', index=False)
X = X.drop('Risk', axis=1)

# Modeling
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
print('\nTrain Shape:', X_train.shape, y_train.shape)
print('Test Shape:', X_test.shape, y_test.shape)

# Initialize models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Create ensemble
ensemble = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='soft')

# Train ensemble
ensemble.fit(X_train, y_train)

# Save model
joblib.dump(ensemble, '/content/Thyroid_Models/ensemble_model.pkl')

# Evaluation
# Predictions
y_pred = ensemble.predict(X_test)

# Accuracy
print('\nAccuracy:', accuracy_score(y_test, y_pred))

# Classification report
print('\nClassification Report:\n', classification_report(y_test, y_pred, target_names=['Low', 'Intermediate', 'High']))

# Confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Intermediate', 'High'], yticklabels=['Low', 'Intermediate', 'High'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig('/content/Thyroid_Plots/confusion_matrix.png', dpi=300)
plt.show()

# Cross-validation
cv_scores = cross_val_score(ensemble, X, y, cv=5, scoring='f1_macro')
print(f'\n5-fold CV F1-macro: {cv_scores.mean():.2f} Â± {cv_scores.std():.2f}')

# Explainability (SHAP)
# Train a RandomForest for SHAP (faster than ensemble)
rf_shap = RandomForestClassifier(n_estimators=100, random_state=42)
rf_shap.fit(X_train, y_train)

# Compute SHAP values
explainer = shap.TreeExplainer(rf_shap)
shap_values = explainer.shap_values(X_test)

# Verify shapes
print('\nX_test Shape:', X_test.shape)
print('SHAP Values Shape:', [v.shape for v in shap_values])

# Save SHAP values
np.save('/content/Thyroid_Models/shap_values.npy', shap_values)

# SHAP summary plots
shap.summary_plot(shap_values, X_test, plot_type='bar', show=False)
plt.savefig('/content/Thyroid_Plots/shap_bar_plot.png', dpi=300)
plt.show()

shap.summary_plot(shap_values, X_test, show=False)
plt.savefig('/content/Thyroid_Plots/shap_summary_plot.png', dpi=300)
plt.show()

"""Week 4: Flask API"""

# Command-Line Prediction Interface
import pandas as pd
import joblib
import pickle
import os

# Load artifacts
model_dir = 'Thyroid_Models'
try:
    model = joblib.load(os.path.join(model_dir, 'ensemble_model.pkl'))
    with open(os.path.join(model_dir, 'label_encoders.pkl'), 'rb') as f:
        label_encoders = pickle.load(f)
    scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
    X_full = pd.read_csv(os.path.join(model_dir, 'encoded_dataset.csv')).drop('Risk', axis=1)
    print('Artifacts loaded successfully.')
except Exception as e:
    print(f'Error loading artifacts: {e}')
    exit()

input_data = {}
print("\n--- Enter New Patient Data for Thyroid Cancer Risk Prediction ---")
for col in X_full.columns:
    if col == 'Age':
        while True:
            try:
                val = float(input(f"Enter value for {col} (e.g., 20-80): "))
                if 20 <= val <= 80:
                    break
                print("Age must be between 20 and 80.")
            except ValueError:
                print("Please enter a valid number.")
        input_data[col] = val
    else:
        options = label_encoders[col].classes_.tolist()
        print(f"Options for {col}: {options}")
        val = input(f"Choose one for {col}: ")
        while val not in options:
            print("Invalid input. Please choose from:", options)
            val = input(f"Choose one for {col}: ")
        input_data[col] = label_encoders[col].transform([val])[0]

# Convert to DataFrame
input_df = pd.DataFrame([input_data])[X_full.columns]

# Scale Age
input_df['Age'] = scaler.transform(input_df[['Age']])

# Predict
prediction = model.predict(input_df)[0]
predicted_label = label_encoders['Risk'].inverse_transform([prediction])[0]
print(f"\nPredicted Risk Level: {predicted_label}")

